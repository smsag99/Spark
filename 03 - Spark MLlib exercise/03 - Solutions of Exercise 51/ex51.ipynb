{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#from pyspark.ml.feature import Tokenizer\n",
    "#from pyspark.ml.feature import StopWordsRemover\n",
    "#from pyspark.ml.feature import HashingTF\n",
    "#from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output folders\n",
    "trainingData = \"data/Ex51/data//trainingData.csv\"\n",
    "unlabeledData = \"data/Ex51/data//unlabeledData.csv\"\n",
    "outputPath = \"res_ex51/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************\n",
    "# Training step\n",
    "# *************************\n",
    "\n",
    "# Create a DataFrame from trainingData.csv\n",
    "# Training data in raw format\n",
    "trainingData = spark.read.load(trainingData,\\\n",
    "                     format=\"csv\",\\\n",
    "                     header=True,\\\n",
    "                     inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|    1|The Spark system ...|\n",
      "|    1|Spark is a new di...|\n",
      "|    0|Turin is a beauti...|\n",
      "|    0|Turin is in the n...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.printSchema()\n",
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Python function that returns the number of words occurring in the input string\n",
    "def countWords(text):\n",
    "    return len(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.countWords(text)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register a UDF function associated with countWords\n",
    "# We explicitly report also the data type of the returned value\n",
    "spark.udf.register(\"countWords\", countWords, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Python function that checks if the input string contain the work \"Spark\"\n",
    "def containsSpark(text):\n",
    "    return text.find(\"Spark\")>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.containsSpark(text)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register a UDF function associated with containsSpark\n",
    "# We explicitly report also the data type of the returned value\n",
    "spark.udf.register(\"containsSpark\", containsSpark, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the attributes label and text and create  two new columns:\n",
    "# numLines and SparkWord\n",
    "newFeaturesDF = trainingData\\\n",
    ".selectExpr(\"label\", \"text\", \"countWords(text)\", \"containsSpark(text)\")\\\n",
    ".withColumnRenamed(\"countWords(text)\", \"numLines\")\\\n",
    ".withColumnRenamed(\"containsSpark(text)\", \"SparkWord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- numLines: integer (nullable = true)\n",
      " |-- SparkWord: boolean (nullable = true)\n",
      "\n",
      "+-----+--------------------+--------+---------+\n",
      "|label|                text|numLines|SparkWord|\n",
      "+-----+--------------------+--------+---------+\n",
      "|    1|The Spark system ...|       7|     true|\n",
      "|    1|Spark is a new di...|       6|     true|\n",
      "|    0|Turin is a beauti...|       5|    false|\n",
      "|    0|Turin is in the n...|       8|    false|\n",
      "+-----+--------------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newFeaturesDF.printSchema()\n",
    "newFeaturesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an assembler to combine \"numLines\" and \"SparkWord\" in a Vector\n",
    "assembler = VectorAssembler(inputCols=[\"numLines\", \"SparkWord\"],\\\n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification model based on the logistic regression algorithm\n",
    "# We can set the values of the parameters of the \n",
    "# Logistic Regression algorithm using the setter methods.\n",
    "lr = LogisticRegression()\\\n",
    ".setMaxIter(10)\\\n",
    ".setRegParam(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline that is used to create the logistic regression\n",
    "# model on the training data.\n",
    "# In this case the pipeline is composed of five steps\n",
    "# - text tokenizer\n",
    "# - stopword removal\n",
    "# - TF-IDF computation (performed in two steps)\n",
    "# - Logistic regression model generation\n",
    "pipeline = Pipeline().setStages([assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the pipeline on the training data to build the \n",
    "# classification model\n",
    "classificationModel = pipeline.fit(newFeaturesDF)\n",
    "\n",
    "# Now, the classification model can be used to predict the class label\n",
    "# of new unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************\n",
    "# Prediction  step\n",
    "# *************************\n",
    "# Read unlabeled data\n",
    "# Create a DataFrame from unlabeledData.csv\n",
    "# Unlabeled data in raw format\n",
    "unlabeledData = spark.read.load(unlabeledData,\\\n",
    "                     format=\"csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlabeledData.printSchema()\n",
    "#unlabeledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFeaturesDFunlabeled = unlabeledData\\\n",
    ".selectExpr(\"label\", \"text\", \"countWords(text)\", \"containsSpark(text)\")\\\n",
    ".withColumnRenamed(\"countWords(text)\", \"numLines\")\\\n",
    ".withColumnRenamed(\"containsSpark(text)\", \"SparkWord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- numLines: integer (nullable = true)\n",
      " |-- SparkWord: boolean (nullable = true)\n",
      "\n",
      "+-----+--------------------+--------+---------+\n",
      "|label|                text|numLines|SparkWord|\n",
      "+-----+--------------------+--------+---------+\n",
      "| null|Spark performs be...|       5|     true|\n",
      "| null|Comparison betwee...|       5|     true|\n",
      "| null|Turin is in Piedmont|       4|    false|\n",
      "+-----+--------------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newFeaturesDFunlabeled.printSchema()\n",
    "newFeaturesDFunlabeled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on unlabeled documents by using the \n",
    "# Transformer.transform() method.\n",
    "# The transform will only use the 'features' columns\n",
    "predictionsDF = classificationModel.transform(newFeaturesDFunlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- numLines: integer (nullable = true)\n",
      " |-- SparkWord: boolean (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+-----+--------------------+--------+---------+---------+--------------------+--------------------+----------+\n",
      "|label|                text|numLines|SparkWord| features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------+---------+---------+--------------------+--------------------+----------+\n",
      "| null|Spark performs be...|       5|     true|[5.0,1.0]|[-3.1272480248757...|[0.04199718899423...|       1.0|\n",
      "| null|Comparison betwee...|       5|     true|[5.0,1.0]|[-3.1272480248757...|[0.04199718899423...|       1.0|\n",
      "| null|Turin is in Piedmont|       4|    false|[4.0,0.0]|[3.19966999960023...|[0.96082185681571...|       0.0|\n",
      "+-----+--------------------+--------+---------+---------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsDF.printSchema()\n",
    "predictionsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned DataFrame has the following schema (attributes)\n",
    "# |-- label: string (nullable = true)\n",
    "# |-- text: string (nullable = true)\n",
    "# |-- words: array (nullable = true)\n",
    "# |    |-- element: string (containsNull = true)\n",
    "# |-- filteredWords: array (nullable = true)\n",
    "# |    |-- element: string (containsNull = true)\n",
    "# |-- rawFeatures: vector (nullable = true)\n",
    "# |-- features: vector (nullable = true)\n",
    "# |-- rawPrediction: vector (nullable = true)\n",
    "# |-- probability: vector (nullable = true)\n",
    "# |-- prediction: double (nullable = false)\n",
    "\n",
    "# Select only the original features (i.e., the value of the original text attribute) and \n",
    "# the predicted class for each record\n",
    "predictions = predictionsDF.select(\"text\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+-----------------------------------+----------+\n",
      "|text                               |prediction|\n",
      "+-----------------------------------+----------+\n",
      "|Spark performs better than Hadoop  |1.0       |\n",
      "|Comparison between Spark and Hadoop|1.0       |\n",
      "|Turin is in Piedmont               |0.0       |\n",
      "+-----------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()\n",
    "predictions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in an HDFS output folder\n",
    "predictions.write.csv(outputPath, header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
